{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pushing all DDH data on GeoWB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from arcgis.gis import GIS\n",
    "import credentials as crd\n",
    "import os\n",
    "import ddh\n",
    "import urllib\n",
    "import zipfile\n",
    "import arcgis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "host = 'ddh1stg.prod.acquia-sites.com'\n",
    "\n",
    "ddh.load(host)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username, password = crd.get_credentials(\"Staging\")\n",
    "gis = GIS(\"https://geosdndev.worldbank.org/portal\", username, password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user, pword = crd.get_credentials(\"Production\")\n",
    "gis = GIS(\"https://geowb.worldbank.org/portal\", user, pword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data from DDH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tags(node_id):\n",
    "    \n",
    "    topics  = {'agriculture and food security': ['agriculture', 'food security'],\n",
    " 'climate change': ['climate change'],\n",
    " 'economic growth': ['economic growth'],\n",
    " 'education': ['education'],\n",
    " 'energy and extractives': ['energy', 'extractives'],\n",
    " 'environment and natural resources': ['environment', 'natural resources'],\n",
    " 'financial sector development': ['financial sector development'],\n",
    " 'fragility, conflict and violence': ['fragility', 'conflict', 'violence'],\n",
    " 'gender': ['gender'],\n",
    " 'health, nutrition and population': ['health', 'nutrition', 'population'],\n",
    " 'information and communication technologies': ['information', 'communication', 'ICT'],\n",
    " 'jobs': ['jobs'],\n",
    " 'macroeconomic and structural policies': ['macroeconomic', 'structural policies'],\n",
    " 'macroeconomic vulnerability and debt': ['macroeconomic vulnerability', 'debt'],\n",
    " 'poverty': ['poverty'],\n",
    " 'private sector development': ['private sector', 'private sector development'],\n",
    " 'public sector management': ['public sector management'],\n",
    " 'public-private partnerships': ['public-private partnerships'],\n",
    " 'social development': ['social development'],\n",
    " 'social protection and labor': ['social protection', 'labor'],\n",
    " 'trade': ['trade'],\n",
    " 'transport': ['trabsport'],\n",
    " 'urban development': ['urban development', 'urban'],\n",
    " 'water': ['water']}\n",
    "    \n",
    "    ds = ddh.dataset.get(node_id)\n",
    "    \n",
    "    tops = topics[ddh.taxonomy.get_keywords('field_topic', ds['field_topic']['und'][0]['tid'])[0]]\n",
    "    \n",
    "    if ds['field_tags']:\n",
    "        for val in ds['field_tags']['und']:\n",
    "            top_val = ddh.taxonomy.get_keywords('field_tags', val['tid'])[0]\n",
    "            tops.append(top_val)\n",
    "            \n",
    "    return tops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_item_properties(datasetId, resourceId):\n",
    "    \n",
    "    ds = ddh.dataset.get(datasetId)\n",
    "    \n",
    "    dj = ddh.dataset.get(resourceId)\n",
    "    \n",
    "    tags = get_tags(datasetId)\n",
    "    \n",
    "    if ds['title'].strip() == dj['title'].strip():\n",
    "        title = dj['title']\n",
    "    else:\n",
    "        title = ds['title']+'__'+dj['title']\n",
    "        \n",
    "    lic = ddh.taxonomy.get_keywords('field_license_wbddh',ds['field_license_wbddh']['und'][0]['tid'])[0]\n",
    "    try:\n",
    "        desc = ds['body']['und'][0]['value']\n",
    "    except:\n",
    "        desc =  \"\"\n",
    "        \n",
    "    item_properties = {\n",
    "            \"description\" : desc,\n",
    "            \"title\" : title,\n",
    "            #\"url\" : dj['path'],\n",
    "            \"tags\" : tags,\n",
    "            \"accessInformation\" : ddh.taxonomy.get_keywords('field_wbddh_data_class',dj['field_wbddh_data_class']['und'][0]['tid'])[0],\n",
    "            \"licenseInfo\" : lic,\n",
    "            \"access\" : ddh.taxonomy.get_keywords('field_wbddh_data_class',dj['field_wbddh_data_class']['und'][0]['tid'])[0]\n",
    "            }\n",
    "    \n",
    "    return item_properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def publish_shp(url,meta, item_properties):\n",
    "    \n",
    "    \n",
    "    if meta['field_format']['und'][0]['tid'] in ['957', '839', '1369']:\n",
    "        \n",
    "        if meta['field_format']['und'][0]['tid'] == '839':\n",
    "            file_type = 'PDF'\n",
    "        elif meta['field_format']['und'][0]['tid'] == '957':\n",
    "            file_type = 'Shapefile'\n",
    "        elif meta['field_format']['und'][0]['tid'] == '1369':\n",
    "            file_type = 'GeoJson'\n",
    "        else:\n",
    "            file_type = None\n",
    "                \n",
    "        if not 'DDH_Datasets' in [folder['title'] for folder in gis.users.me.folders]:\n",
    "            gis.content.create_folder(folder='DDH_Datasets')\n",
    "            print(\"Created DDH_Datasets folder.\")\n",
    "        else:\n",
    "            #print(\"The DDH_Datasets folder already exists.\")\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            if (url.startswith('s3')) and (meta['field_wbddh_data_class']['und'][0]['tid'] == '358'):\n",
    "                url_z = \"http://development-data-hub-s3-public.s3.amazonaws.com/ddhfiles/\" + url.split(':')[1][2:]\n",
    "                try:\n",
    "                    shpfile = gis.content.add({'access':'org', 'type':'{}'.format(file_type)}, url_z, folder='DDH_Datasets')\n",
    "                    shpfile.update(item_properties)\n",
    "                    shpfile.share(org=True)\n",
    "                    published_service = shpfile.publish()\n",
    "                    published_service.update(item_properties)\n",
    "                    published_service.share(org=True)\n",
    "                    return shpfile.id, published_service.id\n",
    "                except Exception as e:\n",
    "                    if e.args[0] == \"Job failed.\":\n",
    "                        gis.content.get(\"{}\".format(shpfile.id)).delete()\n",
    "                        gis.content.get(\"{}\".format(published_service.id)).delete()\n",
    "\n",
    "            elif (url.startswith('s3')) and (meta['field_wbddh_data_class']['und'][0]['tid'] == '359'):\n",
    "                url_z = \"http://development-data-hub-s3-official.s3.amazonaws.com/ddhfiles/\" + url.split(':')[1][2:]\n",
    "                try:\n",
    "                    shpfile = gis.content.add({'access':'org', 'type':'{}'.format(file_type)}, url_z, folder='DDH_Datasets')\n",
    "                    shpfile.update(item_properties)\n",
    "                    shpfile.share(org=True)\n",
    "                    published_service = shpfile.publish()\n",
    "                    published_service.update(item_properties)\n",
    "                    published_service.share(org=True)\n",
    "                    return shpfile.id, published_service.id\n",
    "                except Exception as e:\n",
    "                    if e.args[0] == \"Job failed.\":\n",
    "                        gis.content.get(\"{}\".format(shpfile.id)).delete()\n",
    "                        gis.content.search('title:{}'.format(item_properties['title'].strip()), item_type = 'Feature Service')[0].delete()\n",
    "                        \n",
    "            else:\n",
    "                try:\n",
    "                    shpfile = gis.content.add({'access':'org', 'type':'{}'.format(file_type)}, url, folder='DDH_Datasets')\n",
    "                    shpfile.update(item_properties)\n",
    "                    shpfile.share(org=True)\n",
    "                    published_service = shpfile.publish()\n",
    "                    published_service.update(item_properties)\n",
    "                    published_service.share(org=True)\n",
    "                    return shpfile.id, published_service.id\n",
    "                except Exception as e:\n",
    "                    if e.args[0] == \"Job failed.\":\n",
    "                        gis.content.get(\"{}\".format(shpfile.id)).delete()\n",
    "                        gis.content.search('title:{}'.format(item_properties['title'].strip()), item_type = 'Feature Service')[0].delete()\n",
    "                        \n",
    "        except RuntimeError as e:\n",
    "            if e.args[0] == \"Item '{}' already exists.\\n(Error Code: 409)\".format(url.split(':')[1][2:]):\n",
    "                shpfile = gis.content.search(\"{}\".format(data))[0]\n",
    "                published_service = shpfile.publish()\n",
    "                published_service.share(org=True)\n",
    "                return published_service.id, published_service.url\n",
    "            else:\n",
    "                print(e.args[0])\n",
    "                return None, None\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_lis, fl_list, data_id = [], [], []\n",
    "for k in datasets:\n",
    "    ds = ddh.dataset.get(k)\n",
    "\n",
    "    if (ds['field_wbddh_data_class']['und'][0]['tid'] in ['358', '359']) and (ds['field_resources']):\n",
    "        for i in ds['field_resources'].values():\n",
    "            print('Scnanning {}'.format(k))\n",
    "            try:\n",
    "                for j in i:\n",
    "                    dj = ddh.dataset.get(j['target_id'])\n",
    "                    if dj['field_wbddh_resource_type']['und'][0]['tid'] in ['443', '986']:\n",
    "                        if dj['field_link_api']:\n",
    "                            url = dj['field_link_api']['und'][0]['url']\n",
    "                            url = url.replace(\" \", \"%20\")\n",
    "                            item_prop = get_item_properties(k, j['target_id'])\n",
    "                            dat, fl = publish_shp(url, dj, item_prop)\n",
    "                            data_id.append(k)\n",
    "                            data_lis.append(dat) \n",
    "                            fl_list.append(fl)\n",
    "                        elif dj['field_upload']:\n",
    "                            url = dj['field_upload']['und'][0]['uri']\n",
    "                            url = url.replace(\" \", \"%20\")\n",
    "                            item_prop = get_item_properties(k, j['target_id'])\n",
    "                            dat, fl = publish_shp(url, dj, item_prop)\n",
    "                            data_id.append(k)\n",
    "                            data_lis.append(dat) \n",
    "                            fl_list.append(fl)\n",
    "                        else:\n",
    "                            pass\n",
    "                    elif dj['field_wbddh_resource_type']['und'][0]['tid'] == '983':\n",
    "                        item_prop = get_item_properties(k, j['target_id'])\n",
    "                        item_prop['type'] = 'Feature Service'\n",
    "                        item_prop['url'] = fl_list[int(j['target_id'])]\n",
    "                        temp_data = gis.content.add(item_prop, fl_list[int(j['target_id'])], folder='DDH_Datasets')\n",
    "                        temp_data.share(org=True)\n",
    "                        print(temp_data.id, temp_data.url)\n",
    "            except Exception as e:\n",
    "                #print(\"Exception occurred. Error {}\".format(e.args[0]))\n",
    "                data_id.append(k)\n",
    "                data_lis.append(e.args[0]) \n",
    "                fl_list.append(e.args[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Feature Layer Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arcgis import GIS\n",
    "from arcgis import geometry\n",
    "from arcgis import features as fs\n",
    "from getpass import getpass as pwd\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis = gis.content.search(query = \"title : *(ESA EO4SD-Urban) AND owner:DDHPublisher\", max_items=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating TS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = pd.read_csv(r\"C:\\Users\\wb542830\\OneDrive - WBG\\DEC\\DDH\\API\\RomaniaHub\\FL_back_to_ddh.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.sort_values(by='nid', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_list = ['cannot unpack non-iterable NoneType object',\n",
    " 'list index out of range',\n",
    " \"local variable 'published_service' referenced before assignment\",\n",
    " 'tuple index out of range',\n",
    " 'list indices must be integers or slices, not str',\n",
    "\"'NoneType' object is not iterable\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in set(dat.nid.tolist()):\n",
    "    if (dat[dat.nid == i]['feature_id'].iloc[0] not in err_list):\n",
    "        if (dat[dat.nid == i].shape[0]>1):\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_fl(nid):\n",
    "    import urllib\n",
    "    path = r\"C:\\Users\\wb542830\\OneDrive - WBG\\DEC\\DDH\\API\\RomaniaHub\"\n",
    "    \n",
    "    ds = ddh.dataset.get(nid)\n",
    "    nid = str(nid)\n",
    "    \n",
    "    \n",
    "    lis = []\n",
    "    \n",
    "    for i in ds['field_resources']['und']:\n",
    "        if ddh.dataset.get(i['target_id'])['field_format']['und'][0]['tid'] in ['957']:\n",
    "            lis.append(i['target_id'])\n",
    "    \n",
    "    if len(lis) > 1:\n",
    "        \n",
    "        try:\n",
    "            os.mkdir(os.path.join(path, nid))\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "        \n",
    "        url = []        \n",
    "        for res in lis:\n",
    "            try:\n",
    "                url.append(ddh.dataset.get(res)['field_upload']['und'][0]['uri'])\n",
    "            except:\n",
    "                try:\n",
    "                    url.append(ddh.dataset.get(res)['field_link_api']['und'][0]['url'])\n",
    "                except Exception as e:\n",
    "                    print(res, '::', e)\n",
    "\n",
    "        if ds['field_wbddh_data_class']['und'][0]['tid'] in ['358']:\n",
    "            base_url = \"http://development-data-hub-s3-public.s3.amazonaws.com/ddhfiles/\"\n",
    "        elif ds['field_wbddh_data_class']['und'][0]['tid'] in ['359']:\n",
    "            base_url = \"http://development-data-hub-s3-official.s3.amazonaws.com/ddhfiles/\"\n",
    "        else:\n",
    "            base_url = None\n",
    "\n",
    "        for link in url:\n",
    "            if link.startswith('s3'):\n",
    "                url_z = base_url + link.split(':')[1][2:]\n",
    "                urllib.request.urlretrieve(url_z, os.path.join(path, nid, url_z.split(r'/')[-1]))\n",
    "            else:\n",
    "                url_z = link\n",
    "                urllib.request.urlretrieve(url_z, os.path.join(path, nid, url_z.split(r'/')[-1]))\n",
    "\n",
    "        zip_lis = glob.glob(os.path.join(os.getcwd(), nid)+\"\\*.zip\")\n",
    "\n",
    "        for file in zip_lis:\n",
    "            with zipfile.ZipFile(file, 'r') as zip_ref:\n",
    "                zip_ref.extractall(os.path.join(path, nid))\n",
    "\n",
    "        write_lis = [fn for fn in glob.glob(os.path.join(path, nid)+\"\\*\") if not fn.endswith('.zip')]\n",
    "\n",
    "        with zipfile.ZipFile(os.path.join(path, nid, 'updated_{}.zip'.format(nid)),'w') as zip: \n",
    "            for file in write_lis:\n",
    "                os.chdir(os.path.join(path, nid))\n",
    "                zip.write(os.path.join(file.split(\"\\\\\")[-1])) \n",
    "                os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_item_properties_temp(datasetId):\n",
    "    \n",
    "    ds = ddh.dataset.get(datasetId)\n",
    "    \n",
    "    tags = get_tags(datasetId)\n",
    "    \n",
    "    title = ds['title']\n",
    "\n",
    "        \n",
    "    lic = ddh.taxonomy.get_keywords('field_license_wbddh',ds['field_license_wbddh']['und'][0]['tid'])[0]\n",
    "    try:\n",
    "        desc = ds['body']['und'][0]['value']\n",
    "    except:\n",
    "        desc =  \"\"\n",
    "        \n",
    "    item_properties = {\n",
    "            \"description\" : desc,\n",
    "            \"title\" : title,\n",
    "            #\"url\" : dj['path'],\n",
    "            \"tags\" : tags,\n",
    "            \"accessInformation\" : ddh.taxonomy.get_keywords('field_wbddh_data_class',ds['field_wbddh_data_class']['und'][0]['tid'])[0],\n",
    "            \"licenseInfo\" : lic,\n",
    "            \"access\" : ddh.taxonomy.get_keywords('field_wbddh_data_class',ds['field_wbddh_data_class']['und'][0]['tid'])[0]\n",
    "            }\n",
    "    \n",
    "    return item_properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def publish_fl(nid):\n",
    "    path = r\"C:\\Users\\wb542830\\OneDrive - WBG\\DEC\\DDH\\API\\RomaniaHub\"\n",
    "    nid = str(nid)\n",
    "    if os.path.exists(os.path.join(path, nid)):\n",
    "        ds = ddh.dataset.get(nid)\n",
    "        file_path = os.path.join(path, nid, 'updated_{}.zip'.format(nid))\n",
    "        try:\n",
    "            shpfile = gis.content.add({'access':'org', 'type':'{}'.format('Shapefile')}, file_path, folder='DDH_Datasets')\n",
    "            shpfile.update(get_item_properties_temp(nid))\n",
    "            shpfile.share(org=True)\n",
    "            published_service = shpfile.publish()\n",
    "            published_service.share(org=True)\n",
    "            return shpfile.id, published_service.id\n",
    "        except Exception as e:\n",
    "            print(nid, '::', e.args)\n",
    "            return (0, 0)\n",
    "    else:\n",
    "        return 0,0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodeid_ , shpid_, flid_ = [], [], []\n",
    "for i in list(set(dat.nid.tolist()))[1072:]:\n",
    "#for i in [98349, 98480]:\n",
    "    if (dat[dat.nid == i]['feature_id'].iloc[0] not in err_list):\n",
    "        if (dat[dat.nid == i].shape[0]>1):\n",
    "            update_fl(i)\n",
    "            shid, fid = publish_fl(i)\n",
    "            if fid != 0:\n",
    "                nodeid_.append(i)\n",
    "                shpid_.append(shid)\n",
    "                flid_.append(fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dat_f = pd.read_csv(\"Combined_layers_FL.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_fl_ddh(nid, fl_id):\n",
    "    nid = int(nid)\n",
    "    ds = ddh.dataset.get(nid)\n",
    "\n",
    "    rs = ddh.dataset.rs_template()\n",
    "\n",
    "    rs['title'] = ds['title']+'(Feature Service)'\n",
    "    rs['field_wbddh_data_class'] = ds['field_wbddh_data_class']['und'][0]['tid']\n",
    "    url_fl = gis.content.get(fl_id).url\n",
    "    rs['field_link_api'] = url_fl\n",
    "    ddh.taxonomy.update(rs, {'field_wbddh_resource_type': 'Related Material'})\n",
    "\n",
    "    num = len(ds['field_resources']['und'])\n",
    "\n",
    "    try:\n",
    "        up_ds = ddh.dataset.append_resource(nid, rs, num)\n",
    "        print(\"Updated unique identifier: {0}\".format(up_ds))\n",
    "    except ddh.dataset.APIError as err:\n",
    "        print('ERROR: {}'.format(err.response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing AGOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gis_out = GIS(\"https://geowb.maps.arcgis.com/\", '', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shp = gis_out.content.get(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shp.share(org=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "props = {'description': \"## Overview:\\r\\nGlobal results from [gridfinder](https://github.com/carderne/gridfinder) model, produced by ESMAP based on joint work with Facebook and others. Uses night-time lights, road networks and existing grid network data to predict the location of transmission and distribution lines globally. Validated in several countries with ~70% accuracy at 1 km.\\r\\n\\r\\n## More information:\\r\\nBlog with brief overview: https://blogs.worldbank.org/energy/using-night-lights-map-electrical-grid-infrastructure\\r\\nFull research paper: https://www.nature.com/articles/s41597-019-0347-4\\r\\nVisualization: https://gridfinder.org/\\r\\n\\r\\n## The following data are included:\\r\\n*   **grid.gpkg**: Vectorized predicted distribution and transmission line network, with existing OpenStreetMap lines tagged in the 'source' column\\r\\n*   **targets.tif**: Binary aster showing locations predicted to be connected to distribution grid. \\r\\n*   **lv.tif**: Raster of predicted low-voltage infrastructure in kilometres per cell.\",\n",
    " 'title': 'Derived map of global electricity transmission and distribution lines__grid.gpkg',\n",
    " 'tags': ['energy', 'extractives', 'energydata.info'],\n",
    " 'accessInformation': 'public',\n",
    " 'licenseInfo': 'creative commons attribution 4.0',\n",
    " 'access': 'public'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shp.update(props)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
